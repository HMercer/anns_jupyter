{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Deep Learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Handle matrices\n",
    "import retro  # Retro Environment\n",
    "from skimage import transform  # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray  # Help us to gray our frames\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt  # Display graphs\n",
    "from collections import deque  # Ordered collection with ends\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_inv_src.NNet import *\n",
    "from space_inv_src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our frame is:  Box(210, 160, 3)\n",
      "The action size is :  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "env = retro.make(game=\"SpaceInvaders-Atari2600\")\n",
    "\n",
    "print(\"The size of our frame is: \", env.observation_space)\n",
    "print(\"The action size is : \", env.action_space.n)\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "normalized_frame_size = (110, 84)\n",
    "\n",
    "\n",
    "stack_size = 4\n",
    "\n",
    "stacked_frames = deque(\n",
    "    [np.zeros(normalized_frame_size, dtype=np.int) for i in range(stack_size)], maxlen=4\n",
    ")\n",
    "\n",
    "state_size = [\n",
    "    110,\n",
    "    84,\n",
    "    4,\n",
    "]  # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels)\n",
    "action_size = env.action_space.n  # 8 possible actions\n",
    "learning_rate = 0.00025  # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 50  # Total episodes for training\n",
    "max_steps = 50000  # Max possible steps in an episode\n",
    "batch_size = 64  # Batch size\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0  # exploration probability at start\n",
    "explore_stop = 0.01  # minimum exploration probability\n",
    "decay_rate = 0.00001  # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.9  # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = (\n",
    "    batch_size\n",
    ")  # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000  # Number of experiences the Memory can keep\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4  # Number of frames stacked\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 17:41:45.210259  7184 deprecation_wrapper.py:119] From C:\\Users\\kicjo\\Desktop\\my-anns\\RL\\Space invaders Deep Q learning\\space_inv_src\\NNet.py:11: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0903 17:41:45.226215  7184 deprecation_wrapper.py:119] From C:\\Users\\kicjo\\Desktop\\my-anns\\RL\\Space invaders Deep Q learning\\space_inv_src\\NNet.py:12: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0903 17:41:46.161562  7184 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0903 17:41:46.161562  7184 deprecation.py:323] From C:\\Users\\kicjo\\Desktop\\my-anns\\RL\\Space invaders Deep Q learning\\space_inv_src\\NNet.py:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0903 17:41:46.379513  7184 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0903 17:41:46.688066  7184 deprecation.py:323] From C:\\Users\\kicjo\\Desktop\\my-anns\\RL\\Space invaders Deep Q learning\\space_inv_src\\NNet.py:70: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0903 17:41:47.071478  7184 deprecation_wrapper.py:119] From C:\\Users\\kicjo\\Desktop\\my-anns\\RL\\Space invaders Deep Q learning\\space_inv_src\\NNet.py:84: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_net = DQNetwork(state_size, action_size, learning_rate)\n",
    "memory = init_memory(memory_size,pretrain_length,env,stacked_frames,stack_size,normalized_frame_size,possible_actions)\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "tf.summary.scalar(\"Loss\", neural_net.loss)\n",
    "write_op = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-10a4ad4a4e6c>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-10a4ad4a4e6c>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    if step % 500 == 0\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Initialize the decay rate (that will use to reduce epsilon)\n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            state = env.reset()\n",
    "\n",
    "            # preprocess start and stack frames\n",
    "            state, stacked_frames = stack_frames(\n",
    "                stacked_frames, state, True, stack_size, normalized_frame_size\n",
    "            )\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                if step % 500 == 0:\n",
    "                    print(step)\n",
    "                decay_step += 1\n",
    "\n",
    "                action, explore_probability = predict_action(\n",
    "                    explore_start,\n",
    "                    explore_stop,\n",
    "                    decay_rate,\n",
    "                    decay_step,\n",
    "                    state,\n",
    "                    possible_actions,\n",
    "                    sess,\n",
    "                    neural_net\n",
    "                )\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    next_state = np.zeros((110, 84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(\n",
    "                        stacked_frames,\n",
    "                        next_state,\n",
    "                        False,\n",
    "                        stack_size,\n",
    "                        normalized_frame_size,\n",
    "                    )\n",
    "\n",
    "                    step = max_steps\n",
    "\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print(\n",
    "                        \"Episode: {}\".format(episode),\n",
    "                        \"Total reward: {}\".format(total_reward),\n",
    "                        \"Explore P: {:.4f}\".format(explore_probability),\n",
    "                        \"Training Loss {:.4f}\".format(loss),\n",
    "                    )\n",
    "\n",
    "                    # add empty frame to memotry\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    next_state, stacked_frames = stack_frames(\n",
    "                        stacked_frames,\n",
    "                        next_state,\n",
    "                        False,\n",
    "                        stack_size,\n",
    "                        normalized_frame_size,\n",
    "                    )\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                # here we are calculating Qs of the next step. it is either a Reward or R+gamma+Q of best actions\n",
    "                target_Qs_batch = []\n",
    "                Qs_next_state = sess.run(\n",
    "                    neural_net.output, feed_dict={neural_net.inputs_: next_states_mb}\n",
    "                )\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "\n",
    "\n",
    "                targets_mb = np.array(target_Qs_batch)\n",
    "\n",
    "                # having updated Q values,\n",
    "                loss, _ = sess.run(\n",
    "                    [neural_net.loss, neural_net.optimizer],\n",
    "                    feed_dict={\n",
    "                        neural_net.inputs_: states_mb,\n",
    "                        neural_net.target_Q: targets_mb,\n",
    "                        neural_net.actions_: actions_mb,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                summary = sess.run(\n",
    "                    write_op,\n",
    "                    feed_dict={\n",
    "                        neural_net.inputs_: states_mb,\n",
    "                        neural_net.target_Q: targets_mb,\n",
    "                        neural_net.actions_: actions_mb,\n",
    "                    },\n",
    "                )\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"/models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "\n",
    "    # Load the model\n",
    "    saver.restore(sess, \"/models/model.ckpt\")\n",
    "\n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size,normalized_frame_size)\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            # Reshape the state\n",
    "            state = state.reshape((1, *state_size))\n",
    "            # Get action from Q-network\n",
    "            # Estimate the Qs values state\n",
    "            Qs = sess.run(neural_net.output, feed_dict={neural_net.inputs_: state})\n",
    "\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "\n",
    "            # Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "\n",
    "            next_state, stacked_frames = stack_frames(\n",
    "                stacked_frames, next_state, False, stack_size,normalized_frame_size\n",
    "            )\n",
    "            state = next_state\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
